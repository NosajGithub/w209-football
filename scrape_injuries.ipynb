{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import csv\n",
    "from random import randint\n",
    "from time import sleep\n",
    "import lxml\n",
    "import re\n",
    "\n",
    "def getInjuries(team, year):\n",
    "    \"\"\"\n",
    "    Scrapes one page of injury information\n",
    "    \"\"\"\n",
    "\n",
    "    # Scrape injury information\n",
    "    content = urllib.urlopen(\"http://www.pro-football-reference.com/teams/\" + team + \"/\" + year + \"_injuries.htm\")\n",
    "    s = content.read()\n",
    "#     s = open(\"injuries.html\")\n",
    "\n",
    "    soup = BeautifulSoup(s)\n",
    "\n",
    "    table = soup.find('table')\n",
    "    # print table.prettify()\n",
    "\n",
    "    # Scrape headers into their own table\n",
    "    opponent = []\n",
    "    game = []\n",
    "    date = []\n",
    "    game_url = []\n",
    "    for num, heading in enumerate(table.findAll(\"th\")):\n",
    "    \n",
    "        this_url = heading.find_all(\"a\", href=True)\n",
    "        if (this_url) != []:\n",
    "            game_url.append(this_url[0].get('href'))\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        head = heading.get_text()\n",
    "        game.append(num)\n",
    "        date.append(head[0:5])\n",
    "        opponent.append(head[9:12])\n",
    "\n",
    "    headers = pd.DataFrame({'opponent':opponent, 'game':game, 'date':date, 'game_url':game_url})\n",
    "    \n",
    "    # Scrape body of table\n",
    "    name = []\n",
    "    player_url = []\n",
    "    status = []\n",
    "    played = []\n",
    "    injury = []\n",
    "    game = []\n",
    "\n",
    "    for row in table.findAll(\"tr\"):\n",
    "        cells = row.findAll(\"td\")\n",
    "        if (len(cells) > 0):\n",
    "            for i in range(1,len(cells)):\n",
    "                # Name\n",
    "                name.append(cells[0].get_text())\n",
    "                player_url.append(cells[0].a.get('href'))\n",
    "                \n",
    "                # Game\n",
    "                game.append(i)\n",
    "                \n",
    "                # Status\n",
    "                status.append(cells[i].get_text())\n",
    "                \n",
    "                # Played\n",
    "                if (cells[i].attrs.get('class') is not None):\n",
    "                    played.append(not('played' in cells[i].attrs.get('class')))\n",
    "                else:\n",
    "                    played.append(None)\n",
    "\n",
    "                # Injury detail\n",
    "                if cells[i].find(\"span\") != None:\n",
    "                    inj_type = str.split(cells[i].find(\"span\").attrs.get('tip').encode('utf-8'),\": \")[1]\n",
    "                    injury.append(inj_type)\n",
    "                else:\n",
    "                    injury.append(None)\n",
    "\n",
    "    body = pd.DataFrame({'name':name,'status':status,'injury':injury,'played':played,'game':game,'year':year,'team':team,\n",
    "                         'player_url':player_url})\n",
    "\n",
    "    # Merge together dataframes\n",
    "    both = pd.merge(headers, body, on = 'game')\n",
    "    cols = ['team', 'year', 'game', 'date', 'opponent', 'name', 'status', 'injury', 'played','player_url','game_url']\n",
    "    \n",
    "    both = both[cols].sort(columns=['year','team','name','game'])\n",
    "    \n",
    "    # Scrape roster information\n",
    "    content = urllib.urlopen(\"http://www.pro-football-reference.com/teams/\" + team + \"/\" + year + \"_roster.htm\")\n",
    "    s = content.read()\n",
    "    soup = BeautifulSoup(s)\n",
    "\n",
    "    table = soup.find_all('table')[1]\n",
    "    roster = pd.read_html(str(table))[0]\n",
    "    \n",
    "    new_columns = roster.columns.values\n",
    "    new_columns[1] = 'name'\n",
    "    roster.columns = new_columns\n",
    "    \n",
    "    roster['name'] = [re.sub('[*]|[+]', '', name) for name in roster['name']]\n",
    "\n",
    "    # Merge and return\n",
    "    final = pd.merge(both, roster, how = \"left\", on=\"name\")\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crd 2012\n",
      "crd 2013\n",
      "crd 2014\n",
      "atl 2012\n",
      "atl 2013\n",
      "atl 2014\n",
      "rav 2012\n",
      "rav 2013\n",
      "rav 2014\n",
      "buf 2012\n",
      "buf 2013\n",
      "buf 2014\n",
      "car 2012\n",
      "car 2013\n",
      "car 2014\n",
      "chi 2012\n",
      "chi 2013\n",
      "chi 2014\n",
      "cin 2012\n",
      "cin 2013\n",
      "cin 2014\n",
      "cle 2012\n",
      "cle 2013\n",
      "cle 2014\n",
      "dal 2012\n",
      "dal 2013\n",
      "dal 2014\n",
      "den 2012\n",
      "den 2013\n",
      "den 2014\n",
      "det 2012\n",
      "det 2013\n",
      "det 2014\n",
      "gnb 2012\n",
      "gnb 2013\n",
      "gnb 2014\n",
      "htx 2012\n",
      "htx 2013\n",
      "htx 2014\n",
      "clt 2012\n",
      "clt 2013\n",
      "clt 2014\n",
      "jax 2012\n",
      "jax 2013\n",
      "jax 2014\n",
      "kan 2012\n",
      "kan 2013\n",
      "kan 2014\n",
      "mia 2012\n",
      "mia 2013\n",
      "mia 2014\n",
      "min 2012\n",
      "min 2013\n",
      "min 2014\n",
      "nwe 2012\n",
      "nwe 2013\n",
      "nwe 2014\n",
      "nor 2012\n",
      "nor 2013\n",
      "nor 2014\n",
      "nyg 2012\n",
      "nyg 2013\n",
      "nyg 2014\n",
      "nyj 2012\n",
      "nyj 2013\n",
      "nyj 2014\n",
      "rai 2012\n",
      "rai 2013\n",
      "rai 2014\n",
      "phi 2012\n",
      "phi 2013\n",
      "phi 2014\n",
      "pit 2012\n",
      "pit 2013\n",
      "pit 2014\n",
      "sdg 2012\n",
      "sdg 2013\n",
      "sdg 2014\n",
      "sfo 2012\n",
      "sfo 2013\n",
      "sfo 2014\n",
      "sea 2012\n",
      "sea 2013\n",
      "sea 2014\n",
      "ram 2012\n",
      "ram 2013\n",
      "ram 2014\n",
      "tam 2012\n",
      "tam 2013\n",
      "tam 2014\n",
      "oti 2012\n",
      "oti 2013\n",
      "oti 2014\n",
      "was 2012\n",
      "was 2013\n",
      "was 2014\n"
     ]
    }
   ],
   "source": [
    "## Get all pages to scrape\n",
    "teams = [\"crd\", \"atl\", \"rav\", \"buf\",\"car\",\"chi\",\"cin\",\"cle\",\"dal\",\"den\",\"det\",\"gnb\",\"htx\",\"clt\",\"jax\",\"kan\",\"mia\",\"min\",\n",
    "           \"nwe\",\"nor\",\"nyg\",\"nyj\",\"rai\",\"phi\",\"pit\",\"sdg\",\"sfo\",\"sea\",\"ram\",\"tam\",\"oti\",\"was\"]\n",
    "years = ['2009','2010','2011','2012','2013','2014']\n",
    "# years1 = ['2009','2010','2011']\n",
    "# years2 = ['2012','2013','2014']\n",
    "team_years = [(x,y) for x in teams for y in years]\n",
    "\n",
    "## Scrape each of the pages\n",
    "output = pd.DataFrame()\n",
    "for a,b in team_years:\n",
    "    print a, b\n",
    "    sleep(randint(1,3))\n",
    "    sleep(randint(1,3))    \n",
    "    output = output.append(getInjuries(a,b))\n",
    "\n",
    "# Having some problems with drafted column, so I've taken that out\n",
    "output = output.drop('Drafted (tm/rnd/yr)',1)\n",
    "\n",
    "output.to_csv(\"output.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
